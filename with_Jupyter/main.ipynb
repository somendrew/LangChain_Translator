{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R_Bq24-JV-WT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -qU langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sam_sk import api_key\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = api_key\n",
        "\n"
      ],
      "metadata": {
        "id": "JvtdHO0Wx7km"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "model = ChatGroq(model=\"llama3-8b-8192\")"
      ],
      "metadata": {
        "id": "ThkJZMyVyEu6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages =[\n",
        "    SystemMessage(content = \"English to Italian Translator!\"),\n",
        "    HumanMessage(content = \"hi!\"),\n",
        "    ]\n",
        "\n",
        "model.invoke(messages)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztkW59H5ygXQ",
        "outputId": "7d422700-338c-47a9-bb53-5f00c9fa12a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Ciao! Come stai oggi? (Hello! How are you today?)', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 22, 'total_tokens': 39, 'completion_time': 0.014166667, 'prompt_time': 0.002755013, 'queue_time': 0.011756336, 'total_time': 0.01692168}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None}, id='run-1fb68411-1178-4955-b1fd-96c00eb063e1-0', usage_metadata={'input_tokens': 22, 'output_tokens': 17, 'total_tokens': 39})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "result = model.invoke(messages)\n",
        "parser.invoke(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lD1KDulRzZ_R",
        "outputId": "7a1cf6bf-fc7a-47bc-89be-b40aaeb4f850"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ciao!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The `|` operator is used in LangChain to combine two elements together."
      ],
      "metadata": {
        "id": "ufbPgZQi03Lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Chain the model with output"
      ],
      "metadata": {
        "id": "fsXahc3u1CIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = model | parser\n",
        "chain.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "721MTI530kUU",
        "outputId": "8bb473b4-64fa-4784-adb8-ccc52f16fc4a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ciao!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see that the chain has two steps: first the language model is called, then the result of that is passed to the output parser"
      ],
      "metadata": {
        "id": "uJKmqHT_1dQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates"
      ],
      "metadata": {
        "id": "3VWZEZT01pPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain offers a tool called PromptTemplates that converts the users raw input in such a way model can understand\n",
        "\n",
        "Let's create a PromptTemplate here. It will take in two user variables:\n",
        "\n",
        "* `language:` The language to translate text into\n",
        "\n",
        "* `text:` The text to translate"
      ],
      "metadata": {
        "id": "gqHeJ2nH2lvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "#lets create a string that we will format to be the system msg\n",
        "system_template = \"Translate the following into {language}:\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_template),\n",
        "        (\"user\", \"{text}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "FZGs07Nn1OfV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = prompt_template.invoke({\"language\": \"Italian\", \"text\":\"Hi, Beautiful!\"})\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOPe5ESB4Mqg",
        "outputId": "50993193-f4d1-401c-a357-5aa005a644a4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Translate the following into Italian:', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hi, Beautiful!', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that it returns a `ChatPromptValue` that consists of two messages. If we want to access the messages directly we do:"
      ],
      "metadata": {
        "id": "htSRmzGn6MeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQhGJZ6z6N-x",
        "outputId": "d66a3a92-1d21-49c1-d36d-d76c0c01ad36"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='Translate the following into Italian:', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Hi, Beautiful!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chaining together components with LCEL"
      ],
      "metadata": {
        "id": "NhcgoTx66V1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template | model | parser\n",
        "chain.invoke({\"language\":\"Italian\", \"text\":\"Hi, Beautiful!\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZLu2utfN6ROD",
        "outputId": "c3f0e6dd-bb32-45f6-e180-d39b42a034dd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ciao, Bella!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Serving with LangServe\n",
        "\n",
        "Now that we've built an application, we need to serve it. That's where LangServe comes in.\n"
      ],
      "metadata": {
        "id": "fwo-Jlq09aq_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ghvrEMOq9s3t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}